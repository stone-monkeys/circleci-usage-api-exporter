version: 2.1

parameters:
  org_id:
    type: string
    default: ${CIRCLE_ORGANIZATION_ID}
  cost_per_credit:
    type: string
    default: "0.0006"
  job_name:
    type: string
    default: "unit_tests"

executors:
  python-small:
    docker:
      - image: cimg/python:3.13.1
    resource_class: small
  python-large:
    docker:
      - image: cimg/python:3.13.1
    resource_class: large

commands:
  install-requirements:
    description: "Install Python dependencies from requirements.txt with caching"
    steps:
      - restore_cache:
          keys:
            - pip-deps-v1-{{ checksum "requirements.txt" }}
            - pip-deps-v1-
      - run:
          name: Install dependencies
          command: |
            pip install --user -r requirements.txt
      - save_cache:
          key: pip-deps-v1-{{ checksum "requirements.txt" }}
          paths:
            - ~/.local/lib/python3.13/site-packages
  
  install-all-deps:
    description: "Install all dependencies (requirements + analysis) with caching"
    steps:
      - restore_cache:
          keys:
            - system-deps-v1-{{ checksum "requirements.txt" }}-jupyter
            - system-deps-v1-
      - run:
          name: Install Python dependencies
          command: |
            pip3 install -r requirements.txt
      - run:
          name: Install Jupyter
          command: |
            pip3 install jupyter nbconvert
      - save_cache:
          key: system-deps-v1-{{ checksum "requirements.txt" }}-jupyter
          paths:
            - /usr/local/lib/python3.13/dist-packages
  
  verify-merged-data:
    description: "Verify merged data file exists"
    steps:
      - run:
          name: Verify merged data file exists
          command: |
            ls -la /tmp/merged.csv
            echo "Using merged file directly from workspace at /tmp/merged.csv"
  
  run-analysis:
    description: "Run analysis script with parameters"
    parameters:
      analysis_type:
        type: string
    steps:
      - run:
          name: Execute << parameters.analysis_type >> analysis
          command: |
            cd src
            if [ "<< pipeline.parameters.job_name >>" != "" ]; then
              python run_analysis.py --type << parameters.analysis_type >> --job << pipeline.parameters.job_name >> --credit-cost << pipeline.parameters.cost_per_credit >>
            else
              python run_analysis.py --type << parameters.analysis_type >> --credit-cost << pipeline.parameters.cost_per_credit >>
            fi

jobs:
  generate-usage-report:
    executor: python-small
    steps:
      - checkout
      - install-requirements
      - run:
          environment:
            START_DATE: "2025-07-01"
            END_DATE: "2025-08-01"
            ORG_ID: << pipeline.parameters.org_id >>
          name: Get Usage Report
          command: |
            ORG_ID="$(echo "${ORG_ID}" | circleci env subst)"
            python ./src/get_usage_report.py
      - run:
          name: Merge multiple csv files together and do some sorting 
          command: |
            python ./src/merge.py
      - run:
          name: Create a graph from the merged data 
          command: |
            python ./src/create_graph.py
      - persist_to_workspace:
          root: /tmp/reports
          paths:
            - merged.csv
      - store_artifacts:
          path: /tmp/reports
  
  send-metrics-to-datadog:
    executor: python-small
    steps:
      - checkout
      - attach_workspace:
          at: /tmp
      - install-requirements
      - run:
          name: Send metrics to Datadog
          command: python ./src/send_to_datadog.py /tmp/merged.csv

  generate-resource-class-analysis:
    executor: python-large
    steps:
      - checkout
      - attach_workspace:
          at: /tmp
      - install-all-deps
      - verify-merged-data
      - run-analysis:
          analysis_type: "resource"
      - persist_to_workspace:
          root: /tmp
          paths: reports

  gather-artifacts:
    executor: python-small
    steps:
      - checkout
      - attach_workspace:
          at: /tmp
      - store_artifacts:
          path: /tmp/reports

  send-metrics-to-doit:
    docker:
      - image: cimg/python:3.13.1
    resource_class: small
    steps:
      - checkout
      - attach_workspace:
          at: /tmp
      - run:
          name: Install dependencies
          command: |
            pip3 install -r requirements.txt
      - run:
          name: Send metrics to DoiT
          command: |
            python ./src/send_to_doit.py /tmp/merged.csv

workflows:
  use-usage-api:
    jobs:
      - generate-usage-report:
          context:
            - my-context
      - generate-resource-class-analysis:
          requires:
            - generate-usage-report
      - gather-artifacts:
          requires:
            - generate-resource-class-analysis